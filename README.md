

# SWE-bench Pro Evaluation - OpenLibrary

This repository contains an automated evaluation pipeline for solving a SWE-bench Pro task using AI agents.

## Objective
Build and verify an end-to-end solution using GitHub Actions to automate SWE-bench Pro task evaluation.

**Task ID:** `internetarchive__openlibrary-c4eebe6677acc4629cb541a98d5e91311444f5d4`  
**Agent:** Claude 3.5 Sonnet (Anthropic API)

## Repository Structure
- `.github/workflows/swebench-eval.yml`: The GitHub Actions workflow definition.
- `scripts/run_agent.py`: The main orchestrator that runs the AI agent and applies fixes.
- `scripts/extract_metrics.py`: Script to analyze logs and generate `result.json`.
- `scripts/setup_repository.sh`: Shell script to clone and prepare the target repository environment.
- `scripts/task.yaml`: Metadata about the task, requirements, and evaluation commands.

## Workflow Steps
1. **Setup**: Clones the `openlibrary` repository and checks out the specific environment.
2. **Pre-verification**: Runs existing tests to confirm failure (bug reproduction).
3. **Agent Fix**: Sends the issue description and failure logs to Claude. Claude generates a Git patch.
4. **Apply Fix**: The system applies the patch to the source code.
5. **Post-verification**: Runs the tests again to verify the fix.
6. **Artifact Generation**: Calculates metrics (tokens, time, cost) and generates all required artifacts.

## How to Run
1. Fork this repository.
2. **Add your Anthropic API key as a repository secret:**
   - Go to your repository's Settings > Secrets and variables > Actions
   - Click "New repository secret"
   - Name: `CLAUDE_API_KEY`
   - Value: Your Anthropic API key (starts with `sk-ant-`)
   - Click "Add secret"
3. Trigger the workflow manually via `Actions > SWE-bench Pro Evaluation > run workflow` or push a change to `main`.

## Verification Artifacts
After a successful run, the following artifacts are generated:
- `agent.log`: JSONL log of AI agent actions.
- `result.json`: Metrics and success status.
- `pre_verification.log`: Pytest output before fix.
- `post_verification.log`: Pytest output after fix.
- `changes.patch`: The diff generated by the AI.
- `prompts.md`: All prompts sent to the AI.
